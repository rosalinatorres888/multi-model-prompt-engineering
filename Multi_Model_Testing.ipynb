{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Multi-Model Prompt Engineering Testing Notebook\n",
    "\n",
    "This notebook tests your complete AI setup with **OpenAI**, **Llama**, and **Google Gemini**.\n",
    "\n",
    "## üéØ What This Notebook Does:\n",
    "- ‚úÖ Verifies all API connections\n",
    "- üîÑ Compares responses across models\n",
    "- üß™ Runs prompt engineering experiments\n",
    "- üìä Provides interactive testing environment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Environment Setup and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and load environment\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Install required packages if missing\n",
    "required_packages = ['python-dotenv', 'openai', 'google-generativeai', 'requests']\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        if package == 'python-dotenv':\n",
    "            import dotenv\n",
    "        elif package == 'google-generativeai':\n",
    "            import google.generativeai as genai\n",
    "        else:\n",
    "            __import__(package)\n",
    "        print(f\"‚úÖ {package} is available\")\n",
    "    except ImportError:\n",
    "        print(f\"üì¶ Installing {package}...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "        print(f\"‚úÖ {package} installed\")\n",
    "\n",
    "print(\"\\nüéâ All required packages are ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Try to load .env from multiple locations\n",
    "env_loaded = False\n",
    "for env_path in ['.env', '../.env', '/Users/rosalinatorres/Documents/.env']:\n",
    "    if os.path.exists(env_path):\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"‚úÖ Loaded environment from: {env_path}\")\n",
    "        env_loaded = True\n",
    "        break\n",
    "\n",
    "if not env_loaded:\n",
    "    print(\"‚ùå .env file not found\")\n",
    "    print(\"Please ensure your .env file is in the right location\")\n",
    "else:\n",
    "    # Check API keys\n",
    "    api_keys = {\n",
    "        'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY'),\n",
    "        'LLAMA_API_KEY': os.getenv('LLAMA_API_KEY'),\n",
    "        'GOOGLE_API_KEY': os.getenv('GOOGLE_API_KEY'),\n",
    "        'GOOGLE_API_KEY_BACKUP': os.getenv('GOOGLE_API_KEY_BACKUP')\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüîê API Key Status:\")\n",
    "    for key_name, key_value in api_keys.items():\n",
    "        if key_value and key_value != \"your-openai-key-here\":\n",
    "            masked_key = f\"{key_value[:8]}...{key_value[-8:]}\"\n",
    "            print(f\"‚úÖ {key_name}: {masked_key}\")\n",
    "        else:\n",
    "            print(f\"‚ùå {key_name}: Not configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Initialize Multi-Model Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotebookMultiModelPlatform:\n",
    "    \"\"\"Simplified multi-model platform for Jupyter notebooks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.openai_key = os.getenv('OPENAI_API_KEY')\n",
    "        self.llama_key = os.getenv('LLAMA_API_KEY')\n",
    "        self.google_key = os.getenv('GOOGLE_API_KEY')\n",
    "        self.google_backup_key = os.getenv('GOOGLE_API_KEY_BACKUP')\n",
    "        \n",
    "        self.clients = {}\n",
    "        self.setup_clients()\n",
    "    \n",
    "    def setup_clients(self):\n",
    "        \"\"\"Initialize API clients\"\"\"\n",
    "        print(\"üîß Setting up API clients...\")\n",
    "        \n",
    "        # OpenAI\n",
    "        if self.openai_key and self.openai_key != \"your-openai-key-here\":\n",
    "            try:\n",
    "                from openai import OpenAI\n",
    "                self.clients['openai'] = OpenAI(api_key=self.openai_key)\n",
    "                print(\"‚úÖ OpenAI client ready\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå OpenAI setup failed: {e}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  OpenAI key not configured\")\n",
    "        \n",
    "        # Google Gemini\n",
    "        google_keys = [self.google_key, self.google_backup_key]\n",
    "        google_keys = [k for k in google_keys if k]  # Remove None values\n",
    "        \n",
    "        for i, key in enumerate(google_keys):\n",
    "            try:\n",
    "                import google.generativeai as genai\n",
    "                genai.configure(api_key=key)\n",
    "                model = genai.GenerativeModel('gemini-pro')\n",
    "                \n",
    "                # Test with minimal request\n",
    "                test_response = model.generate_content(\n",
    "                    \"Hi\", generation_config={'max_output_tokens': 5}\n",
    "                )\n",
    "                \n",
    "                self.clients['google'] = model\n",
    "                key_type = \"Primary\" if i == 0 else \"Backup\"\n",
    "                print(f\"‚úÖ Google Gemini ready ({key_type} key)\")\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                key_type = \"Primary\" if i == 0 else \"Backup\"\n",
    "                print(f\"‚ùå Google {key_type} key failed: {str(e)[:50]}\")\n",
    "                continue\n",
    "        \n",
    "        if self.llama_key:\n",
    "            print(\"‚úÖ Llama key available (HTTP-based)\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Llama key not configured\")\n",
    "    \n",
    "    def test_openai(self, prompt):\n",
    "        \"\"\"Test OpenAI API\"\"\"\n",
    "        if 'openai' not in self.clients:\n",
    "            return \"‚ùå OpenAI not available\"\n",
    "        \n",
    "        try:\n",
    "            response = self.clients['openai'].chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=100\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå OpenAI Error: {str(e)[:100]}\"\n",
    "    \n",
    "    def test_llama(self, prompt):\n",
    "        \"\"\"Test Llama API\"\"\"\n",
    "        if not self.llama_key:\n",
    "            return \"‚ùå Llama not available\"\n",
    "        \n",
    "        try:\n",
    "            url = \"https://api.llama.com/v1/chat/completions\"\n",
    "            headers = {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Authorization\": f\"Bearer {self.llama_key}\"\n",
    "            }\n",
    "            data = {\n",
    "                \"model\": \"Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"max_tokens\": 100\n",
    "            }\n",
    "            \n",
    "            response = requests.post(url, headers=headers, json=data, timeout=30)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                return result['choices'][0]['message']['content']\n",
    "            else:\n",
    "                return f\"‚ùå Llama API Error: {response.status_code}\"\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Llama Error: {str(e)[:100]}\"\n",
    "    \n",
    "    def test_google(self, prompt):\n",
    "        \"\"\"Test Google Gemini API\"\"\"\n",
    "        if 'google' not in self.clients:\n",
    "            return \"‚ùå Google not available\"\n",
    "        \n",
    "        try:\n",
    "            response = self.clients['google'].generate_content(\n",
    "                prompt,\n",
    "                generation_config={'max_output_tokens': 100}\n",
    "            )\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Google Error: {str(e)[:100]}\"\n",
    "    \n",
    "    def compare_models(self, prompt, title=\"Model Comparison\"):\n",
    "        \"\"\"Compare responses across all models\"\"\"\n",
    "        print(f\"\\nüéØ {title}\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        models = {\n",
    "            \"OpenAI GPT-3.5\": self.test_openai,\n",
    "            \"Meta Llama\": self.test_llama,\n",
    "            \"Google Gemini\": self.test_google\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        for model_name, test_func in models.items():\n",
    "            print(f\"\\nü§ñ {model_name}:\")\n",
    "            print(\"-\" * 30)\n",
    "            response = test_func(prompt)\n",
    "            print(response)\n",
    "            results[model_name] = response\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize the platform\n",
    "platform = NotebookMultiModelPlatform()\n",
    "print(\"\\nüéâ Multi-Model Platform Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Connection Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick connection test\n",
    "test_prompt = \"Hello! Respond with just 'API working!' to test the connection.\"\n",
    "results = platform.compare_models(test_prompt, \"üß™ Connection Test\")\n",
    "\n",
    "# Count working services\n",
    "working_services = [name for name, response in results.items() if not response.startswith(\"‚ùå\")]\n",
    "print(f\"\\nüìä Summary: {len(working_services)}/3 services are working\")\n",
    "\n",
    "if len(working_services) > 0:\n",
    "    print(\"‚úÖ You're ready for prompt engineering experiments!\")\n",
    "else:\n",
    "    print(\"‚ùå Please check your API keys and internet connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Prompt Engineering Experiments\n",
    "\n",
    "Now let's run some prompt engineering experiments to see how different models respond!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Mathematical Reasoning\n",
    "math_prompt = \"If I have 20 marbles and give away 7, then find 5 more, how many marbles do I have now? Show your reasoning.\"\n",
    "platform.compare_models(math_prompt, \"üßÆ Mathematical Reasoning Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Creative Writing\n",
    "creative_prompt = \"Write a short poem about coffee in the morning.\"\n",
    "platform.compare_models(creative_prompt, \"üé® Creative Writing Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Code Generation\n",
    "code_prompt = \"Write a Python function that reverses a string.\"\n",
    "platform.compare_models(code_prompt, \"üíª Code Generation Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: Reasoning and Analysis\n",
    "reasoning_prompt = \"What are the pros and cons of remote work? Give me 3 points for each.\"\n",
    "platform.compare_models(reasoning_prompt, \"üîç Reasoning and Analysis Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Interactive Testing\n",
    "\n",
    "Use this cell to test your own prompts across all models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prompt testing\n",
    "# Change this prompt to test your own ideas!\n",
    "\n",
    "your_prompt = \"Explain artificial intelligence in simple terms that a 12-year-old could understand.\"\n",
    "\n",
    "# Run the comparison\n",
    "platform.compare_models(your_prompt, \"üéÆ Your Custom Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Advanced Prompt Engineering Techniques\n",
    "\n",
    "Let's test some advanced prompting techniques!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain-of-Thought Prompting\n",
    "cot_prompt = \"\"\"\n",
    "A restaurant bill comes to $120. If I want to tip 18%, how much should I tip?\n",
    "\n",
    "Let me think step by step:\n",
    "\"\"\"\n",
    "\n",
    "platform.compare_models(cot_prompt, \"üîó Chain-of-Thought Prompting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-Shot Learning Example\n",
    "few_shot_prompt = \"\"\"\n",
    "Classify these movie reviews as positive or negative:\n",
    "\n",
    "Review: \"This movie was amazing! Great acting and plot.\"\n",
    "Classification: Positive\n",
    "\n",
    "Review: \"Boring and predictable. Waste of time.\"\n",
    "Classification: Negative\n",
    "\n",
    "Review: \"Incredible cinematography and compelling characters.\"\n",
    "Classification: Positive\n",
    "\n",
    "Review: \"The story dragged on and the ending was disappointing.\"\n",
    "Classification:\n",
    "\"\"\"\n",
    "\n",
    "platform.compare_models(few_shot_prompt, \"üìö Few-Shot Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Summary and Next Steps\n",
    "\n",
    "üéâ **Congratulations!** You now have a complete multi-model prompt engineering environment!\n",
    "\n",
    "### What you can do next:\n",
    "- ‚úèÔ∏è Modify the prompts above to test your own ideas\n",
    "- üî¨ Experiment with different prompting techniques\n",
    "- üìä Compare how different models respond to the same prompt\n",
    "- üìö Use this setup with your prompt engineering course materials\n",
    "\n",
    "### Pro Tips:\n",
    "- Start with simple prompts and gradually make them more complex\n",
    "- Notice which models excel at different types of tasks\n",
    "- Try the same prompt with different phrasings\n",
    "- Use the models' different strengths for different parts of your projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final verification - let's make sure everything is working\n",
    "print(\"üèÅ FINAL SETUP VERIFICATION\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Quick test of each service\n",
    "services_status = {\n",
    "    \"OpenAI\": not platform.test_openai(\"Hi\").startswith(\"‚ùå\"),\n",
    "    \"Llama\": not platform.test_llama(\"Hi\").startswith(\"‚ùå\"),\n",
    "    \"Google\": not platform.test_google(\"Hi\").startswith(\"‚ùå\")\n",
    "}\n",
    "\n",
    "working_count = sum(services_status.values())\n",
    "total_count = len(services_status)\n",
    "\n",
    "for service, is_working in services_status.items():\n",
    "    status = \"‚úÖ\" if is_working else \"‚ùå\"\n",
    "    print(f\"{status} {service}\")\n",
    "\n",
    "print(f\"\\nüìä {working_count}/{total_count} services working\")\n",
    "\n",
    "if working_count == total_count:\n",
    "    print(\"üéâ PERFECT! All systems operational!\")\n",
    "    print(\"You have the ultimate prompt engineering setup!\")\n",
    "elif working_count > 0:\n",
    "    print(\"‚úÖ GOOD! You can start prompt engineering!\")\n",
    "    print(f\"You have {working_count} working AI model(s) to experiment with.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No services working. Please check your .env file and API keys.\")\n",
    "\n",
    "print(\"\\nüöÄ Ready for prompt engineering adventures!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
