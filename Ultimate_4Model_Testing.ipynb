{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåü ULTIMATE 4-Model Prompt Engineering Testing Notebook\n",
    "\n",
    "## üéØ The Most Comprehensive AI Testing Environment\n",
    "\n",
    "This notebook gives you access to **ALL 4 major AI providers**:\n",
    "- üß† **OpenAI GPT-4 & GPT-3.5** - Industry standard\n",
    "- üé≠ **Anthropic Claude Sonnet** - Advanced reasoning  \n",
    "- üîç **Google Gemini** - Google's flagship\n",
    "- ü¶ô **Meta Llama** - Open source powerhouse\n",
    "\n",
    "## üöÄ What This Enables:\n",
    "- ‚úÖ **4-way model comparisons** on the same prompt\n",
    "- üî¨ **Advanced prompt engineering** techniques\n",
    "- üìä **Model strength analysis** across different domains\n",
    "- üé® **Creative vs analytical** task comparison\n",
    "- üí∞ **Cost/quality optimization** across providers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Environment Setup and Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages for the ultimate setup\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "ultimate_packages = [\n",
    "    'python-dotenv',\n",
    "    'openai', \n",
    "    'anthropic',\n",
    "    'google-generativeai',\n",
    "    'requests'\n",
    "]\n",
    "\n",
    "print(\"üîß Installing Ultimate AI Package Suite...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for package in ultimate_packages:\n",
    "    try:\n",
    "        if package == 'python-dotenv':\n",
    "            import dotenv\n",
    "        elif package == 'google-generativeai':\n",
    "            import google.generativeai as genai\n",
    "        elif package == 'anthropic':\n",
    "            import anthropic\n",
    "        else:\n",
    "            __import__(package)\n",
    "        print(f\"‚úÖ {package:<25}: Already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"üì¶ {package:<25}: Installing...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "        print(f\"‚úÖ {package:<25}: Installed successfully\")\n",
    "\n",
    "print(\"\\nüéâ Ultimate package suite ready!\")\n",
    "print(\"You now have access to all 4 major AI providers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load Environment and Verify API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables and check API key status\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Try to load .env from multiple locations\n",
    "env_loaded = False\n",
    "env_paths = ['.env', '../.env', '/Users/rosalinatorres/Documents/.env']\n",
    "\n",
    "for env_path in env_paths:\n",
    "    if os.path.exists(env_path):\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"‚úÖ Loaded environment from: {env_path}\")\n",
    "        env_loaded = True\n",
    "        break\n",
    "\n",
    "if not env_loaded:\n",
    "    print(\"‚ùå .env file not found\")\n",
    "    print(\"Please ensure your .env file is in the right location\")\n",
    "else:\n",
    "    # Check all 5 API keys\n",
    "    ultimate_keys = {\n",
    "        'OPENAI_API_KEY': 'OpenAI GPT',\n",
    "        'CLAUDE_API_KEY': 'Claude Sonnet',  \n",
    "        'GOOGLE_API_KEY': 'Google Gemini (Primary)',\n",
    "        'GOOGLE_API_KEY_BACKUP': 'Google Gemini (Backup)',\n",
    "        'LLAMA_API_KEY': 'Meta Llama'\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüîê ULTIMATE API Key Status:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    configured_count = 0\n",
    "    for key_name, description in ultimate_keys.items():\n",
    "        key_value = os.getenv(key_name)\n",
    "        if key_value and key_value != \"your-openai-key-here\":\n",
    "            if len(key_value) > 20:\n",
    "                masked_key = f\"{key_value[:8]}...{key_value[-8:]}\"\n",
    "            else:\n",
    "                masked_key = f\"{key_value[:6]}...{key_value[-6:]}\"\n",
    "            print(f\"‚úÖ {description:<25}: {masked_key}\")\n",
    "            configured_count += 1\n",
    "        else:\n",
    "            print(f\"‚ùå {description:<25}: Not configured\")\n",
    "    \n",
    "    print(f\"\\nüìä Ultimate Summary: {configured_count}/5 API keys configured\")\n",
    "    \n",
    "    if configured_count == 5:\n",
    "        print(\"üéâ PERFECT! All 5 API keys configured!\")\n",
    "        print(\"You have the ULTIMATE prompt engineering setup!\")\n",
    "    elif configured_count >= 3:\n",
    "        print(\"üî• EXCELLENT! Multiple AI providers ready!\")\n",
    "    elif configured_count >= 2:\n",
    "        print(\"‚úÖ GOOD! You can run model comparisons!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Limited setup - add more API keys for full functionality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Initialize Ultimate 4-Model Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UltimateNotebookPlatform:\n",
    "    \"\"\"Ultimate 4-model platform optimized for Jupyter notebooks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.openai_key = os.getenv('OPENAI_API_KEY')\n",
    "        self.claude_key = os.getenv('CLAUDE_API_KEY')\n",
    "        self.google_key = os.getenv('GOOGLE_API_KEY')\n",
    "        self.google_backup_key = os.getenv('GOOGLE_API_KEY_BACKUP')\n",
    "        self.llama_key = os.getenv('LLAMA_API_KEY')\n",
    "        \n",
    "        self.clients = {}\n",
    "        self.setup_all_clients()\n",
    "    \n",
    "    def setup_all_clients(self):\n",
    "        \"\"\"Initialize all available AI clients\"\"\"\n",
    "        print(\"üîß Initializing Ultimate AI Platform...\")\n",
    "        print(\"=\" * 45)\n",
    "        \n",
    "        # OpenAI Setup\n",
    "        if self.openai_key and self.openai_key != \"your-openai-key-here\":\n",
    "            try:\n",
    "                from openai import OpenAI\n",
    "                self.clients['openai'] = OpenAI(api_key=self.openai_key)\n",
    "                print(\"‚úÖ OpenAI GPT client ready\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå OpenAI setup failed: {e}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  OpenAI key not configured\")\n",
    "        \n",
    "        # Claude Setup\n",
    "        if self.claude_key:\n",
    "            try:\n",
    "                import anthropic\n",
    "                self.clients['claude'] = anthropic.Anthropic(api_key=self.claude_key)\n",
    "                print(\"‚úÖ Claude Sonnet client ready\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Claude setup failed: {e}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Claude key not configured\")\n",
    "        \n",
    "        # Google Gemini Setup (with backup key fallback)\n",
    "        google_keys = [self.google_key, self.google_backup_key]\n",
    "        google_keys = [k for k in google_keys if k]\n",
    "        \n",
    "        for i, key in enumerate(google_keys):\n",
    "            try:\n",
    "                import google.generativeai as genai\n",
    "                genai.configure(api_key=key)\n",
    "                model = genai.GenerativeModel('gemini-pro')\n",
    "                \n",
    "                # Test connection\n",
    "                test_response = model.generate_content(\n",
    "                    \"Test\", generation_config={'max_output_tokens': 5}\n",
    "                )\n",
    "                \n",
    "                self.clients['google'] = model\n",
    "                key_type = \"Primary\" if i == 0 else \"Backup\"\n",
    "                print(f\"‚úÖ Google Gemini ready ({key_type} key)\")\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                key_type = \"Primary\" if i == 0 else \"Backup\"\n",
    "                print(f\"‚ùå Google {key_type} key failed: {str(e)[:50]}\")\n",
    "                continue\n",
    "        \n",
    "        # Llama Setup (HTTP-based)\n",
    "        if self.llama_key:\n",
    "            print(\"‚úÖ Meta Llama key configured (HTTP-based)\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Llama key not configured\")\n",
    "        \n",
    "        # Summary\n",
    "        available_models = len([k for k in self.clients.keys()]) + (1 if self.llama_key else 0)\n",
    "        print(f\"\\nüöÄ Platform Status: {available_models}/4 AI models ready\")\n",
    "        \n",
    "        if available_models == 4:\n",
    "            print(\"üéâ ULTIMATE SETUP COMPLETE! All 4 AI providers ready!\")\n",
    "        elif available_models >= 2:\n",
    "            print(f\"üî• GREAT! {available_models} AI providers ready for comparisons!\")\n",
    "        elif available_models == 1:\n",
    "            print(\"‚úÖ Basic setup ready - you can start experimenting!\")\n",
    "    \n",
    "    # Individual model response methods\n",
    "    def get_openai_response(self, prompt, system_prompt=None, model=\"gpt-4\"):\n",
    "        \"\"\"Get response from OpenAI\"\"\"\n",
    "        if 'openai' not in self.clients:\n",
    "            return \"‚ùå OpenAI not available\"\n",
    "        \n",
    "        try:\n",
    "            messages = []\n",
    "            if system_prompt:\n",
    "                messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "            messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "            \n",
    "            response = self.clients['openai'].chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                max_tokens=400,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå OpenAI Error: {str(e)[:100]}\"\n",
    "    \n",
    "    def get_claude_response(self, prompt, system_prompt=None, model=\"claude-3-5-sonnet-20241022\"):\n",
    "        \"\"\"Get response from Claude\"\"\"\n",
    "        if 'claude' not in self.clients:\n",
    "            return \"‚ùå Claude not available\"\n",
    "        \n",
    "        try:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            \n",
    "            kwargs = {\n",
    "                \"model\": model,\n",
    "                \"max_tokens\": 400,\n",
    "                \"temperature\": 0.7,\n",
    "                \"messages\": messages\n",
    "            }\n",
    "            \n",
    "            if system_prompt:\n",
    "                kwargs[\"system\"] = system_prompt\n",
    "            \n",
    "            response = self.clients['claude'].messages.create(**kwargs)\n",
    "            return response.content[0].text\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Claude Error: {str(e)[:100]}\"\n",
    "    \n",
    "    def get_google_response(self, prompt, system_prompt=None):\n",
    "        \"\"\"Get response from Google Gemini\"\"\"\n",
    "        if 'google' not in self.clients:\n",
    "            return \"‚ùå Google not available\"\n",
    "        \n",
    "        try:\n",
    "            full_prompt = prompt\n",
    "            if system_prompt:\n",
    "                full_prompt = f\"Instructions: {system_prompt}\\n\\nUser: {prompt}\"\n",
    "            \n",
    "            response = self.clients['google'].generate_content(\n",
    "                full_prompt,\n",
    "                generation_config={\n",
    "                    'max_output_tokens': 400,\n",
    "                    'temperature': 0.7\n",
    "                }\n",
    "            )\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Google Error: {str(e)[:100]}\"\n",
    "    \n",
    "    def get_llama_response(self, prompt, system_prompt=None):\n",
    "        \"\"\"Get response from Llama\"\"\"\n",
    "        if not self.llama_key:\n",
    "            return \"‚ùå Llama not available\"\n",
    "        \n",
    "        try:\n",
    "            url = \"https://api.llama.com/v1/chat/completions\"\n",
    "            headers = {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Authorization\": f\"Bearer {self.llama_key}\"\n",
    "            }\n",
    "            \n",
    "            messages = []\n",
    "            if system_prompt:\n",
    "                messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "            messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "            \n",
    "            data = {\n",
    "                \"model\": \"Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "                \"messages\": messages,\n",
    "                \"max_tokens\": 400,\n",
    "                \"temperature\": 0.7\n",
    "            }\n",
    "            \n",
    "            response = requests.post(url, headers=headers, json=data, timeout=45)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                return result['choices'][0]['message']['content']\n",
    "            else:\n",
    "                return f\"‚ùå Llama API Error: {response.status_code}\"\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Llama Error: {str(e)[:100]}\"\n",
    "    \n",
    "    def ultimate_4model_comparison(self, prompt, system_prompt=None, title=\"üåü Ultimate 4-Model Comparison\"):\n",
    "        \"\"\"Compare responses across all 4 AI models\"\"\"\n",
    "        print(f\"\\n{title}\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        if system_prompt:\n",
    "            print(f\"System: {system_prompt}\")\n",
    "        print(\"=\" * 75)\n",
    "        \n",
    "        models = {\n",
    "            \"üß† OpenAI GPT-4\": lambda: self.get_openai_response(prompt, system_prompt),\n",
    "            \"üé≠ Claude Sonnet\": lambda: self.get_claude_response(prompt, system_prompt),\n",
    "            \"üîç Google Gemini\": lambda: self.get_google_response(prompt, system_prompt),\n",
    "            \"ü¶ô Meta Llama\": lambda: self.get_llama_response(prompt, system_prompt)\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for model_name, model_func in models.items():\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            print(\"-\" * 50)\n",
    "            response = model_func()\n",
    "            print(response)\n",
    "            results[model_name] = response\n",
    "            print()\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize the ultimate platform\n",
    "ultimate_platform = UltimateNotebookPlatform()\n",
    "print(\"\\nüåü Ultimate 4-Model Platform Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Ultimate Connection Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all 4 AI providers with a simple prompt\n",
    "test_prompt = \"Hello! Respond with just 'API working - [Your Model Name]!' to test the connection.\"\n",
    "\n",
    "results = ultimate_platform.ultimate_4model_comparison(\n",
    "    test_prompt, \n",
    "    \"Be concise and just respond with the requested format.\",\n",
    "    \"üß™ Ultimate Connection Test\"\n",
    ")\n",
    "\n",
    "# Analyze results\n",
    "working_models = [name for name, response in results.items() if not response.startswith(\"‚ùå\")]\n",
    "print(f\"\\nüìä Connection Test Summary: {len(working_models)}/4 models working\")\n",
    "\n",
    "if len(working_models) == 4:\n",
    "    print(\"üéâ PERFECT! All 4 AI providers are working!\")\n",
    "    print(\"You have the ultimate prompt engineering setup!\")\n",
    "elif len(working_models) >= 2:\n",
    "    print(f\"üî• EXCELLENT! {len(working_models)} models working - you can run comprehensive comparisons!\")\n",
    "elif len(working_models) == 1:\n",
    "    print(\"‚úÖ Good! 1 model working - you can start experimenting!\")\n",
    "else:\n",
    "    print(\"‚ùå No models working. Please check your API keys and internet connection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Ultimate Prompt Engineering Experiments\n",
    "\n",
    "Now let's run comprehensive experiments across all 4 AI providers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Mathematical Reasoning Comparison\n",
    "math_prompt = \"\"\"A rectangle has a length that is 3 times its width. If the perimeter is 24 cm, \n",
    "what are the dimensions? Show your step-by-step reasoning.\"\"\"\n",
    "\n",
    "ultimate_platform.ultimate_4model_comparison(\n",
    "    math_prompt, \n",
    "    \"Think step-by-step and show all calculations clearly.\",\n",
    "    \"üßÆ Mathematical Reasoning Comparison\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Creative Writing Showdown\n",
    "creative_prompt = \"Write a haiku about the relationship between humans and AI.\"\n",
    "\n",
    "ultimate_platform.ultimate_4model_comparison(\n",
    "    creative_prompt,\n",
    "    \"Be creative and thoughtful. Follow the traditional 5-7-5 syllable pattern.\",\n",
    "    \"üé® Creative Writing Showdown\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Code Generation Challenge\n",
    "code_prompt = \"\"\"Write a Python function that finds the second largest number in a list. \n",
    "Handle edge cases like empty lists and lists with duplicate values.\"\"\"\n",
    "\n",
    "ultimate_platform.ultimate_4model_comparison(\n",
    "    code_prompt,\n",
    "    \"Write clean, efficient code with proper error handling and comments.\",\n",
    "    \"üíª Code Generation Challenge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: Logical Reasoning Test\n",
    "logic_prompt = \"\"\"If all roses are flowers, and some flowers fade quickly, \n",
    "can we logically conclude that some roses fade quickly? Explain your reasoning using formal logic.\"\"\"\n",
    "\n",
    "ultimate_platform.ultimate_4model_comparison(\n",
    "    logic_prompt,\n",
    "    \"Use precise logical reasoning. Identify premises, conclusion, and explain validity.\",\n",
    "    \"üß† Logical Reasoning Test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 5: Analysis & Synthesis Challenge\n",
    "analysis_prompt = \"\"\"Compare the advantages and disadvantages of renewable energy vs. fossil fuels. \n",
    "Provide 3 key points for each side and a balanced conclusion.\"\"\"\n",
    "\n",
    "ultimate_platform.ultimate_4model_comparison(\n",
    "    analysis_prompt,\n",
    "    \"Be objective, balanced, and provide evidence-based reasoning.\",\n",
    "    \"üîç Analysis & Synthesis Challenge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Advanced Prompt Engineering Techniques\n",
    "\n",
    "Let's test advanced prompting strategies across all 4 models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain-of-Thought Prompting Test\n",
    "cot_prompt = \"\"\"A bakery sells cupcakes for $3 each. If they offer a 15% discount for orders over 10 cupcakes, \n",
    "how much would it cost to buy 12 cupcakes?\n",
    "\n",
    "Let me work through this step by step:\n",
    "1. First, I'll calculate the base cost\n",
    "2. Then check if the discount applies\n",
    "3. Calculate the final price\n",
    "\n",
    "Step 1:\"\"\"\n",
    "\n",
    "ultimate_platform.ultimate_4model_comparison(\n",
    "    cot_prompt,\n",
    "    \"Continue the step-by-step reasoning and show all calculations.\",\n",
    "    \"üîó Chain-of-Thought Prompting\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-Shot Learning Example\n",
    "few_shot_prompt = \"\"\"Classify the sentiment of these product reviews as positive, negative, or neutral:\n",
    "\n",
    "Review: \"This product exceeded my expectations! Amazing quality and fast shipping.\"\n",
    "Sentiment: Positive\n",
    "\n",
    "Review: \"The item was okay, nothing special but it works as described.\"\n",
    "Sentiment: Neutral\n",
    "\n",
    "Review: \"Terrible quality, broke after one day. Complete waste of money.\"\n",
    "Sentiment: Negative\n",
    "\n",
    "Review: \"Great customer service and the product works perfectly for my needs.\"\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "ultimate_platform.ultimate_4model_comparison(\n",
    "    few_shot_prompt,\n",
    "    \"Follow the pattern shown in the examples to classify the sentiment.\",\n",
    "    \"üìö Few-Shot Learning Test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Role-Based Prompting\n",
    "role_prompt = \"\"\"Explain quantum computing to a curious 12-year-old who loves video games and is good at math.\"\"\"\n",
    "\n",
    "ultimate_platform.ultimate_4model_comparison(\n",
    "    role_prompt,\n",
    "    \"You are an excellent science teacher who makes complex topics fun and relatable. Use gaming analogies where helpful.\",\n",
    "    \"üé≠ Role-Based Prompting\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Interactive Testing Zone\n",
    "\n",
    "Use this section to test your own prompts across all 4 models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prompt testing - modify these variables to test your own prompts!\n",
    "\n",
    "# YOUR CUSTOM PROMPT HERE:\n",
    "your_prompt = \"What are the key principles of effective leadership in the modern workplace?\"\n",
    "\n",
    "# YOUR CUSTOM SYSTEM PROMPT (optional):\n",
    "your_system_prompt = \"Provide practical, actionable advice based on current leadership research.\"\n",
    "\n",
    "# Run the ultimate comparison\n",
    "ultimate_platform.ultimate_4model_comparison(\n",
    "    your_prompt, \n",
    "    your_system_prompt if your_system_prompt else None,\n",
    "    \"üéØ Your Custom 4-Model Test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Test Cell - Change this prompt to test anything you want!\n",
    "\n",
    "quick_prompt = \"Explain the concept of machine learning using an analogy to cooking.\"\n",
    "\n",
    "ultimate_platform.ultimate_4model_comparison(\n",
    "    quick_prompt,\n",
    "    \"Use creative analogies and make it easy to understand.\",\n",
    "    \"‚ö° Quick Test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Model Strength Analysis\n",
    "\n",
    "Let's analyze which models excel at different types of tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Strength Analysis Function\n",
    "def analyze_model_strengths():\n",
    "    \"\"\"Analyze which models perform best at different task types\"\"\"\n",
    "    \n",
    "    strength_tests = {\n",
    "        \"üßÆ Mathematical Reasoning\": \"Solve: 2x¬≤ - 8x + 6 = 0. Show your work using the quadratic formula.\",\n",
    "        \"üé® Creative Expression\": \"Write a creative metaphor comparing artificial intelligence to a natural phenomenon.\",\n",
    "        \"üíª Technical Problem Solving\": \"Debug this Python code: def fibonacci(n): return fibonacci(n-1) + fibonacci(n)\",\n",
    "        \"üß† Logical Analysis\": \"Evaluate this argument: 'Since all programmers use computers, and John uses computers, John must be a programmer.'\",\n",
    "        \"üìä Data Interpretation\": \"If sales increased 15% in Q1, decreased 8% in Q2, and increased 12% in Q3, what's the overall change?\"\n",
    "    }\n",
    "    \n",
    "    print(\"üìä ULTIMATE MODEL STRENGTH ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Testing each model's performance across different domains...\\n\")\n",
    "    \n",
    "    for domain, test_prompt in strength_tests.items():\n",
    "        print(f\"Testing: {domain}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        results = ultimate_platform.ultimate_4model_comparison(\n",
    "            test_prompt,\n",
    "            \"Provide a clear, accurate, and well-reasoned response.\",\n",
    "            f\"üìä {domain}\"\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 75 + \"\\n\")\n",
    "\n",
    "# Run the analysis\n",
    "print(\"üî¨ Running comprehensive model strength analysis...\")\n",
    "print(\"This will test all 4 models across 5 different domains.\\n\")\n",
    "\n",
    "# Uncomment the line below to run the full analysis\n",
    "# analyze_model_strengths()\n",
    "\n",
    "print(\"üí° Tip: Uncomment the analyze_model_strengths() call above to run the full analysis!\")\n",
    "print(\"(It's commented out by default to save on API costs during initial testing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Final Verification and Summary\n",
    "\n",
    "Let's verify your ultimate setup is working perfectly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive verification\n",
    "print(\"üèÅ FINAL ULTIMATE SETUP VERIFICATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test each model individually\n",
    "models_to_test = {\n",
    "    \"OpenAI GPT-4\": lambda: ultimate_platform.get_openai_response(\"Test\"),\n",
    "    \"Claude Sonnet\": lambda: ultimate_platform.get_claude_response(\"Test\"),\n",
    "    \"Google Gemini\": lambda: ultimate_platform.get_google_response(\"Test\"),\n",
    "    \"Meta Llama\": lambda: ultimate_platform.get_llama_response(\"Test\")\n",
    "}\n",
    "\n",
    "working_models = []\n",
    "total_models = len(models_to_test)\n",
    "\n",
    "print(\"üîç Individual Model Status:\")\n",
    "for model_name, test_func in models_to_test.items():\n",
    "    try:\n",
    "        response = test_func()\n",
    "        if not response.startswith(\"‚ùå\"):\n",
    "            print(f\"‚úÖ {model_name:<20}: Working\")\n",
    "            working_models.append(model_name)\n",
    "        else:\n",
    "            print(f\"‚ùå {model_name:<20}: Not available\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {model_name:<20}: Error - {str(e)[:30]}\")\n",
    "\n",
    "# Final summary\n",
    "working_count = len(working_models)\n",
    "print(f\"\\nüìä Ultimate Setup Summary: {working_count}/{total_models} AI models operational\")\n",
    "\n",
    "if working_count == 4:\n",
    "    print(\"\\nüéâ ULTIMATE SUCCESS!\")\n",
    "    print(\"üåü All 4 major AI providers are working perfectly!\")\n",
    "    print(\"üöÄ You have the most comprehensive prompt engineering environment possible!\")\n",
    "    print(\"\\nüéì What you can now do:\")\n",
    "    print(\"   ‚Ä¢ Compare responses across OpenAI, Claude, Gemini, and Llama\")\n",
    "    print(\"   ‚Ä¢ Test prompt robustness across different AI architectures\")\n",
    "    print(\"   ‚Ä¢ Analyze model strengths and weaknesses\")\n",
    "    print(\"   ‚Ä¢ Run advanced prompt engineering experiments\")\n",
    "    print(\"   ‚Ä¢ Build sophisticated AI applications with model diversity\")\n",
    "    print(\"   ‚Ä¢ Optimize cost vs. quality across different providers\")\n",
    "    \n",
    "elif working_count >= 2:\n",
    "    print(f\"\\nüî• EXCELLENT SETUP!\")\n",
    "    print(f\"‚úÖ {working_count} AI providers working - perfect for model comparisons!\")\n",
    "    print(\"üéØ You can run comprehensive prompt engineering experiments!\")\n",
    "    \n",
    "elif working_count == 1:\n",
    "    print(f\"\\n‚úÖ BASIC SETUP READY!\")\n",
    "    print(\"üéØ 1 AI provider working - you can start prompt engineering!\")\n",
    "    print(\"üí° Add more API keys to enable model comparisons\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  SETUP NEEDS ATTENTION\")\n",
    "    print(\"‚ùå No AI models are working\")\n",
    "    print(\"üîß Please check your .env file and API keys\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"üéì ULTIMATE PROMPT ENGINEERING PLATFORM READY!\")\n",
    "print(\"Happy experimenting with the best AI models available! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåü Congratulations!\n",
    "\n",
    "You now have the **ULTIMATE prompt engineering environment** with access to all 4 major AI providers!\n",
    "\n",
    "### üéØ Next Steps:\n",
    "1. **Experiment freely** - Modify the prompts above to test your own ideas\n",
    "2. **Compare models** - See how different AIs approach the same problem\n",
    "3. **Analyze strengths** - Discover which models excel at different tasks\n",
    "4. **Optimize strategies** - Build prompts that work well across all models\n",
    "\n",
    "### üí° Pro Tips:\n",
    "- **Start simple** and gradually increase complexity\n",
    "- **Notice patterns** in how different models respond\n",
    "- **Test edge cases** to understand model limitations\n",
    "- **Document insights** for future prompt engineering projects\n",
    "\n",
    "### üöÄ You're Ready For:\n",
    "- Advanced prompt engineering courses\n",
    "- Professional AI application development\n",
    "- Research and experimentation\n",
    "- Building sophisticated AI-powered solutions\n",
    "\n",
    "**You now have the same tools used by leading AI researchers and developers! üéì**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.9.12\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}
